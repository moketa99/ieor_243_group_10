{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f7760a9",
   "metadata": {},
   "source": [
    "# IEOR 243 Group 10 Demo\n",
    "\n",
    "#### Target Audience and Primary Objective\n",
    "\n",
    "\n",
    "Suppose we are a data analytics service provider for the Centers for Disease Control and Prevention (CDC) or some other public health organization. \n",
    "\n",
    "Our client's primary objective is to <u>gain insight into public sentiment, misinformation, and key topics of discussion surrounding COVID-19</u>, which will help them tailor communication strategies and public health campaigns more effectively.\n",
    "\n",
    "\n",
    "#### High-Level Model Summary\n",
    "\n",
    "Our modeling approach has four main steps to it:\n",
    "\n",
    "1) Convert all the tweet text to numbers representing the different words/tokens. These are called <u>word embeddings</u>\n",
    "    \n",
    "2) Use a <u>clustering model</u> to group the tweets together into 8 different categories, based on the content of the tweet.\n",
    "\n",
    "3) Use a version of ChatGPT to sample 50-100 tweets from each cluster and describe the tweet using 5-10 keywords\n",
    "- Now, we have a list of 40 labels\n",
    "4) Train a <u>Multioutput</u> classification model to assign labels to each tweet in our dataset.\n",
    "- We trained many types: logistic regression, linear discriminant analysis, multilayer perceptrons, random forests, gradient boosting, etc\n",
    "    \n",
    "\n",
    "#### High-Level Metric Summary\n",
    "\n",
    "The primary metric we use to score models is the <u>Hamming Loss</u>, which the proportion of labels that are *incorrectly* assigned. For instance, a score of 0.05 means 95% of labels were correctly identified (similar to accuracy).\n",
    "\n",
    "We selected the models with the lowest hamming loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dac422a7",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from wordcloud import WordCloud\n",
    "from typing import Union, List, Callable\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import re\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a671181b",
   "metadata": {},
   "source": [
    "## Part 1: Interactive EDA\n",
    "\n",
    "Before using any machine learning, it's often helpful to run ad hoc exploratory data analysis to get a general sense of the data. We want to give the CDC the ability to get macro-trends from the tweet data for the following concepts. \n",
    "\n",
    "1) Most common words in tweets \n",
    "2) Length of tweets\n",
    "3) Number of tweets\n",
    "\n",
    "Because COVID-19 was a global pandemic, it's important to give our client the choice of where to focus their future analysis. \n",
    "\n",
    "For instance, if we see different trends in different locations at different times, this can advice the CDC to aim their messaging differently to different locations.\n",
    "\n",
    "Everything plot type can be filtered by **Date Range** and **Location**\n",
    "\n",
    "##### Examples\n",
    "\n",
    "- Visualize how many #COVID19 tweets are coming from the United States vs other countries\n",
    "\n",
    "\n",
    "- What are the most common words being used in tweets in the United States in March 2020? \n",
    "    - What about August 2020?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "071ef43d",
   "metadata": {
    "code_folding": [
     0,
     1,
     39,
     62,
     81,
     127,
     156
    ]
   },
   "outputs": [],
   "source": [
    "# Preprocessing Helper Functions\n",
    "def preprocess_text(text: str) -> str:\n",
    "    \n",
    "    \"\"\"\n",
    "    Processes a tweet string by removing any weird string characters/formattings\n",
    "    Args: \n",
    "        - text (str): the text to clean\n",
    "    Returns: \n",
    "        - clean_text (str): the cleaned text string\n",
    "    \"\"\"\n",
    "    # convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # remove URLs\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "\n",
    "    # Removing Emojis\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "    \n",
    "    # Removing emoticons\n",
    "    text = re.sub(r':\\w+:', '', text)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "    \n",
    "    # Removing Contractions\n",
    "    text = re.sub(r\"can\\'t\", \"can not\", text)\n",
    "    text = re.sub(r\"won\\'t\", \"will not\", text)\n",
    "    text = re.sub(r\"n\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'s\", \" is\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'m\", \" am\", text)\n",
    "    \n",
    "    clean_text = text\n",
    "    \n",
    "    return clean_text\n",
    "\n",
    "def preprocess_nulls(df: pd.DataFrame) -> pd.DataFrame: \n",
    "    \n",
    "    \"\"\"\n",
    "    Removes nulls and 0 counts from a dataframe\n",
    "    Args: \n",
    "        - df (pd.DataFrame): the dataframe to remove nulls from\n",
    "    Returns: \n",
    "        - clean_df (str): the cleaned df\n",
    "    \"\"\"\n",
    "    \n",
    "    # Drop duplicate rows \n",
    "    df = df.drop_duplicates(subset = \"text\")\n",
    "    \n",
    "    # Drop rows with no followers \n",
    "    df = df[df['user_followers'] > 0]\n",
    "    \n",
    "    # Drop nulls and reset index \n",
    "    df = df.dropna().reset_index(drop = True)\n",
    "    \n",
    "    clean_df = df\n",
    "    \n",
    "    return clean_df\n",
    "\n",
    "def preprocess_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \n",
    "    \"\"\"\n",
    "    Main processing function on the dataframe\n",
    "    Args: \n",
    "        - df (pd.DataFrame): df of tweets to process\n",
    "    Returns: \n",
    "        - preprocessed_df (pd.DataFrame): the processed df\n",
    "    \"\"\"\n",
    "    \n",
    "    # Preprocess null and missing values \n",
    "    df = preprocess_nulls(df)\n",
    "    \n",
    "    # Preprocess text \n",
    "    df['processed_text'] = df['text'].apply(preprocess_text)            \n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def create_widget(): \n",
    "    \n",
    "    \"\"\"\n",
    "    Creates the widget options and returns them\n",
    "    \"\"\"\n",
    "    \n",
    "    # Dropdown menu to choose the plot\n",
    "    plot_options = ['Bar Plot of Most Common Words in Tweets', 'Distribution of Length of Tweets', 'Time-series Plot of Tweet Counts', 'Word Cloud of Most Common Words']\n",
    "    plot_dropdown = widgets.Dropdown(\n",
    "        options=plot_options,\n",
    "        value=plot_options[0],\n",
    "        description='Select Plot:',\n",
    "    )\n",
    "\n",
    "    # Dropdown menu to choose the country\n",
    "    country_options = ['All Countries', 'United States', 'Canada', 'South Africa','Switzerland','London','India','United Kingdom']\n",
    "    country_dropdown = widgets.Dropdown(\n",
    "        options=country_options,\n",
    "        value=country_options[0],\n",
    "        description='Country:',\n",
    "    )\n",
    "\n",
    "    # Date range picker\n",
    "    start_date_picker = widgets.DatePicker(\n",
    "        description='Start Date',\n",
    "        disabled=False\n",
    "    )\n",
    "\n",
    "    end_date_picker = widgets.DatePicker(\n",
    "        description='End Date',\n",
    "        disabled=False\n",
    "    )\n",
    "\n",
    "\n",
    "    # Button to process the dataset and generate the plot\n",
    "    process_button = widgets.Button(\n",
    "        description='Plot',\n",
    "        tooltip='Plot',\n",
    "    )\n",
    "\n",
    "    # Output widget to display the result\n",
    "    output = widgets.Output()\n",
    "    \n",
    "    \n",
    "    return plot_dropdown, country_dropdown, start_date_picker, end_date_picker, process_button, output\n",
    "\n",
    "def load_process_data(file_path): \n",
    "    \n",
    "    \"\"\"\n",
    "    Loads data and logs if successful\"\"\"\n",
    "    \n",
    "    try: \n",
    "        df = pd.read_csv(file_path)\n",
    "    except: \n",
    "        print('Could not load data')\n",
    "        return\n",
    "\n",
    "    # Convert to date\n",
    "    df['date'] = pd.to_datetime(df['date']).dt.date\n",
    "\n",
    "\n",
    "    # Log the output\n",
    "    min_date = df['date'].min().strftime('%m/%d/%Y')\n",
    "    max_date = df['date'].max().strftime('%m/%d/%Y')\n",
    "    print(\"\\nData is loaded successfully\")\n",
    "    print(f'   Earliest Tweet Date: {min_date}')\n",
    "    print(f'   Latest Tweet Date: {max_date}')\n",
    "\n",
    "\n",
    "    # Preprocess dataset\n",
    "    preprocessed_df = preprocess_df(df)\n",
    "    print(\"\\nYour data is ready for analysis.\")\n",
    "    \n",
    "    return preprocessed_df\n",
    "\n",
    "def filter_df(preprocessed_df, country_dropdown, start_date_picker, end_date_picker): \n",
    "    \n",
    "    \"\"\"\n",
    "    Filters the DF based on rules from the dropdowns and option widgets\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filter tweets by country\n",
    "    selected_country = country_dropdown.value\n",
    "    if selected_country != 'All Countries':\n",
    "        filtered_df = preprocessed_df.loc[preprocessed_df['user_location'] == selected_country]\n",
    "    else:\n",
    "        filtered_df = preprocessed_df\n",
    "\n",
    "\n",
    "    filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n",
    "\n",
    "    start_date = start_date_picker.value\n",
    "    end_date = end_date_picker.value\n",
    "    if start_date and end_date:\n",
    "        start_date = pd.to_datetime(start_date)\n",
    "        end_date = pd.to_datetime(end_date)\n",
    "        filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n",
    "        \n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3dfbad6a",
   "metadata": {
    "code_folding": [
     0,
     1,
     22,
     32,
     43,
     56
    ]
   },
   "outputs": [],
   "source": [
    "# Plotting Helper Functions\n",
    "def generate_bar_plot(filtered_df): \n",
    "    \n",
    "    \"\"\"\n",
    "    Bar plot\n",
    "    \"\"\"\n",
    "    # code for bar plot\n",
    "    text = \" \".join(filtered_df['processed_text'])\n",
    "    words = text.split()\n",
    "    words_counter = Counter(words)\n",
    "    most_common_words = words_counter.most_common(20)\n",
    "\n",
    "    words = [word[0] for word in most_common_words]\n",
    "    counts = [word[1] for word in most_common_words]\n",
    "\n",
    "    plt.bar(words, counts)\n",
    "    plt.xlabel('Words')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Bar Plot of Most Common Words in Tweets')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()\n",
    "\n",
    "def generate_tweet_length_plot(filtered_df): \n",
    "    \n",
    "    # code for length distribution plot\n",
    "    filtered_df['text_length'] = filtered_df['text'].apply(len)\n",
    "    filtered_df['text_length'].plot.hist(bins=30, rwidth=0.9)\n",
    "    plt.xlabel('Length of Tweets')\n",
    "    plt.ylabel('Counts')\n",
    "    plt.title('Distribution of Length of Tweets')\n",
    "    plt.show()\n",
    "\n",
    "def generate_time_series_plot(filtered_df): \n",
    "    # code for time-series plot\n",
    "        filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n",
    "        df_grouped = filtered_df.groupby(filtered_df['date'].dt.date).count()\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(df_grouped.index, df_grouped['text'])\n",
    "        ax.set_ylabel('Number of Tweets')\n",
    "        ax.set_title('Time-series Plot of Tweet Counts')\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.show()\n",
    "\n",
    "def generate_word_cloud(filtered_df): \n",
    "    \n",
    "    # code for word cloud\n",
    "        text = \" \".join(filtered_df['processed_text'])\n",
    "        words = text.split()\n",
    "        words_counter = Counter(words)\n",
    "        wordcloud = WordCloud(width=800, height=400).generate_from_frequencies(words_counter)\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis(\"off\")\n",
    "        plt.title('Word Cloud of Most Common Words')\n",
    "        plt.show()\n",
    "\n",
    "def generate_word_cloud_by_location(filtered_df): \n",
    "    \n",
    "    top_words_by_location = {}\n",
    "    for location in filtered_df['user_location'].unique():\n",
    "        location_df = filtered_df.loc[filtered_df['user_location'] == location]\n",
    "        text = \" \".join(location_df['processed_text'])\n",
    "        words = text.split()\n",
    "        words_counter = Counter(words)\n",
    "        most_common_words = words_counter.most_common(20)\n",
    "        top_words_by_location[location] = most_common_words\n",
    "        \n",
    "        \n",
    "     # Plot word cloud for each location\n",
    "    for location, top_words in top_words_by_location.items():\n",
    "        words = [word[0] for word in top_words]\n",
    "        frequencies = [word[1] for word in top_words]\n",
    "        wordcloud = WordCloud(width=800, height=400).generate_from_frequencies(dict(zip(words, frequencies)))\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(f'Most Common Words in Tweets from {location}')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9bacd526",
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter the path to your CSV file: ../data/covid2020.csv\n",
      "\n",
      "Data is loaded successfully\n",
      "   Earliest Tweet Date: 07/24/2020\n",
      "   Latest Tweet Date: 08/30/2020\n",
      "\n",
      "Your data is ready for analysis.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a19ffe78a1e84f188ecb03b92d2741ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Country:', options=('All Countries', 'United States', 'Canada', 'South Africa', 'Switzer…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3159cdd905524b06a20542e02a26fe10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "DatePicker(value=None, description='Start Date', step=1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52369d1c98f74089b0ad2b4f3c7a4326",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "DatePicker(value=None, description='End Date', step=1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "152a5ff801664737b2f27ce8f0704728",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Select Plot:', options=('Bar Plot of Most Common Words in Tweets', 'Distribution of Leng…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3ad078b92874e109b5d2280d43807b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Plot', style=ButtonStyle(), tooltip='Plot')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "605da46a43b54fd99a8981d0ffef0df9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Demo\n",
    "\n",
    "\n",
    "# Load and process data\n",
    "file_path = input(\"Please enter the path to your CSV file: \")\n",
    "preprocessed_df = load_process_data(file_path)\n",
    "\n",
    "if preprocessed_df is not None:\n",
    "\n",
    "    # Create the widget\n",
    "    plot_dropdown, country_dropdown, start_date_picker, end_date_picker, process_button, output = create_widget()\n",
    "\n",
    "\n",
    "    def on_button_click(b):\n",
    "        with output:\n",
    "            output.clear_output()\n",
    "\n",
    "            # Filter DF based on dropdown / options \n",
    "            filtered_df = filter_df(preprocessed_df, country_dropdown, start_date_picker, end_date_picker)\n",
    "\n",
    "            # Plot selected graph\n",
    "            selected_plot = plot_dropdown.value\n",
    "\n",
    "            if selected_plot == 'Bar Plot of Most Common Words in Tweets':\n",
    "                generate_bar_plot(filtered_df)\n",
    "            elif selected_plot == 'Distribution of Length of Tweets':\n",
    "                generate_tweet_length_plot(filtered_df)\n",
    "            elif selected_plot == 'Time-series Plot of Tweet Counts':\n",
    "                generate_time_series_plot(filtered_df)\n",
    "            elif selected_plot == 'Word Cloud of Most Common Words':\n",
    "                generate_word_cloud(filtered_df)\n",
    "            else: # 'Word Cloud of Most Common Words by Location'\n",
    "                generate_word_cloud_by_location(filtered_df)\n",
    "\n",
    "\n",
    "    process_button.on_click(on_button_click)\n",
    "\n",
    "    #Display widgets\n",
    "\n",
    "    display(country_dropdown)\n",
    "    display(start_date_picker)\n",
    "    display(end_date_picker)\n",
    "    display(plot_dropdown)\n",
    "    display(process_button)\n",
    "    display(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2066ce4a",
   "metadata": {},
   "source": [
    "## Part 2: Real Time Inference\n",
    "\n",
    "After analysis has been done, it's important to let our clients use the models for more recent data. This project took a few months, and many more #COVID19 tweets have been created since then, with new protocols, vaccine information, regulation, etc.\n",
    "\n",
    "We want to give the ability to the health organization to see trends in individual tweets.\n",
    "\n",
    "More importantly, the organization needs to be given the ability to choose which model they want. Even thouh there is similar performance between the best two models, they can give slightly different results. The analysts at the organization should be able to choose ad hoc.\n",
    "\n",
    "#### Sample Tweets to try:\n",
    "\n",
    "- #COVID19 cases are on the rise again. Let's all do our part to stop the spread: get vaccinated, wear a mask, and social distance. Together, we can beat this virus.\n",
    "- Indian municipalities have been a great failure in controlling #COVID19 , barring a few.\n",
    "- Coronavirus infections top half a million in South Africa... #SouthAfrica #Gauteng #Pretoria #COVID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "abd8f0c1",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Load in necessary data \n",
    "\n",
    "# Load in embedding model \n",
    "embedding_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "# Load in trained models \n",
    "lr = pickle.load(open('../trained_models/lr_v2.pkl', 'rb'))\n",
    "lda = pickle.load(open('../trained_models/l_v2.pkl', 'rb'))\n",
    "kmeans = pickle.load(open('../trained_models/kmeans.pkl', 'rb'))\n",
    "\n",
    "# Grab the labels from GPT API from Module 2\n",
    "labels_dict = {\n",
    "    0: ['social issues', 'personal development', 'business and economics', 'community building'],\n",
    "    1: ['india', 'updates', 'testing', 'fatalities', 'recoveries', 'healthcare'],\n",
    "    2: ['face masks', 'safety', 'protection', 'public health', 'prevention'],\n",
    "    3: ['social media', 'resilience', 'community support', 'online events'],\n",
    "    4: ['global', 'cases', 'deaths', 'statistics'],\n",
    "    5: ['politics', 'government response', 'public health', 'conspiracy', 'human rights'],\n",
    "    6: ['health', 'information','vacccine', 'public awareness'],\n",
    "    7: ['layoffs', 'misinofrmation', 'mental health', 'lockdown', 'access', 'financial impact', 'political response', 'education']\n",
    "}\n",
    "\n",
    "# Grab the labels from GPT API from Module 2\n",
    "labels = ['social-issues',\n",
    " 'personal-development',\n",
    " 'business-and-economics',\n",
    " 'community-building',\n",
    " 'india',\n",
    " 'updates',\n",
    " 'testing',\n",
    " 'fatalities',\n",
    " 'recoveries',\n",
    " 'healthcare',\n",
    " 'face-masks',\n",
    " 'safety',\n",
    " 'protection',\n",
    " 'public-health',\n",
    " 'prevention',\n",
    " 'social-media',\n",
    " 'resilience',\n",
    " 'community-support',\n",
    " 'online-events',\n",
    " 'global',\n",
    " 'cases',\n",
    " 'deaths',\n",
    " 'statistics',\n",
    " 'politics',\n",
    " 'government-response',\n",
    " 'conspiracy',\n",
    " 'human-rights',\n",
    " 'health',\n",
    " 'information',\n",
    " 'vacccine',\n",
    " 'public-awareness',\n",
    " 'layoffs',\n",
    " 'misinformation',\n",
    " 'mental-health',\n",
    " 'lockdown',\n",
    " 'access',\n",
    " 'financial-impact',\n",
    " 'political-response',\n",
    " 'education']\n",
    "\n",
    "def make_prediction(tweet: str, \n",
    "                    labels: Union[dict, list],\n",
    "                    embedding_model: SentenceTransformer, \n",
    "                    classification_model = None,\n",
    "                    clustering_model = None): \n",
    "    \n",
    "    \"\"\"\n",
    "    Generate a list of predicted labels for an input tweet\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate the word embedding \n",
    "    embedding = embedding_model.encode(tweet)\n",
    "    embedding = embedding.reshape(1, -1).astype(float)\n",
    "    \n",
    "    # If both there -> return nothing because error\n",
    "    if (clustering_model) and (classification_model):\n",
    "        return None\n",
    "    \n",
    "    # If clustering -> use that\n",
    "    if (clustering_model):\n",
    "        prediction = clustering_model.predict(embedding)\n",
    "        return labels[prediction[0]]\n",
    "    \n",
    "    if (classification_model):\n",
    "        prediction = classification_model.predict(embedding)[0]\n",
    "        return [labels[i] for i in range(len(labels)) if prediction[i] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5d01cf3a",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a tweet: Coronavirus infections top half a million in South Africa... #SouthAfrica #Gauteng #Pretoria #COVID\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c91ee7e5c4b94c3c8c7c2a804654b5b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Select Model Type: ', index=1, options=('KMeans + GPT API', 'Logistic Regression', 'Line…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e17f5c6406e743ac88649bafa7354479",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Run Inference', style=ButtonStyle(), tooltip='Run Inference')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f64cf4b727be41928506273375433eb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Demo\n",
    "\n",
    "# Tweet Input\n",
    "tweet = input(\"Enter a tweet: \")\n",
    "\n",
    "# Select model type \n",
    "options = ['KMeans + GPT API', 'Logistic Regression', 'Linear Discriminant Analysis']\n",
    "dropdown = widgets.Dropdown(options = options,\n",
    "                            value = options[1],\n",
    "                            description = 'Select Model Type: ')\n",
    "\n",
    "# Button to process the dataset and generate the plot\n",
    "button = widgets.Button(description = 'Run Inference', tooltip = 'Run Inference')\n",
    "\n",
    "# Display Result\n",
    "output = widgets.Output()\n",
    "\n",
    "def run_inference(labels, labels_dict, embedding_model, kmeans, lr): \n",
    "    \n",
    "    with output: \n",
    "        \n",
    "        # Clear any existing display\n",
    "        output.clear_output()\n",
    "    \n",
    "        # If clustering\n",
    "        if dropdown.value == options[0]: \n",
    "            labels = make_prediction(tweet = tweet, \n",
    "                                     labels = labels_dict, \n",
    "                                     embedding_model = embedding_model,\n",
    "                                     clustering_model = kmeans, \n",
    "                                     classification_model = None)\n",
    "            \n",
    "        else: \n",
    "            labels = make_prediction(tweet = tweet, \n",
    "                                     labels = labels,\n",
    "                                     embedding_model = embedding_model,\n",
    "                                     clustering_model = None, \n",
    "                                     classification_model = lr)\n",
    "            \n",
    "        # Print output\n",
    "        print(labels)\n",
    "\n",
    "button.on_click(lambda _: run_inference(labels, labels_dict, embedding_model, kmeans, lr))\n",
    "\n",
    "# Display everything \n",
    "display(dropdown)\n",
    "display(button)\n",
    "display(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140efc9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
