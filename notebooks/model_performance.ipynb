{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75b6e0a1",
   "metadata": {},
   "source": [
    "# Model Performance\n",
    "\n",
    "In this notebook, we load in our trained models and compute the performance for each of the models for comparison.\n",
    "\n",
    "Because we are doing a multilabel (not multiclass) classification problem, we must be smart with the metrics we should use. While accuracy is generally fine with binary classification problems, it isn't ideal for this. \n",
    "\n",
    "For instance, suppose a target label is [1, 1, 0, 0, 1] meaning the first two labels and the last label apply. If a model predicts [1, 1, 0, 0, 0], we argue this is a fairly good model because it got 4/5 correct. However, the vectors are not the same so if accuracy were our primary metric, this would be a score of 0. In short, accuracy is not a detailed enough metric to classify performance.\n",
    "\n",
    "Instead, we chose to use the **Hamming Loss**, which is a measure $\\in [0, 1]$ representing the proportion of incorrect labels. In the example above, $HL = 0.2$ because 1 of the 5 labels was incorrect.\n",
    "\n",
    "During hyperparameter tuning, we still decided to leave the scoring metric to be accuracy because increasing accuracy will always lead to better Hamming Loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea094f6a",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Callable, Union\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, hamming_loss\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multioutput import MultiOutputClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34785fc9",
   "metadata": {},
   "source": [
    "### Running KMeans to Label Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "157c29fb",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Load in the embeddings \n",
    "path = \"../data/X20_embeddings.csv.zip\"\n",
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "799ead82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24676, 768)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42191714",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Train the Kmeans clustering model with 8 clusters (from Module 2)\n",
    "kmeans = KMeans(n_clusters = 8, random_state = 1)\n",
    "kmeans = kmeans.fit(df)\n",
    "\n",
    "# Grab the labels from GPT API from Module 2\n",
    "labels = {\n",
    "    0: ['social-issues', 'personal-development', 'business-and-economics', 'community-building'],\n",
    "    1: ['india', 'updates', 'testing', 'fatalities', 'recoveries', 'healthcare'],\n",
    "    2: ['face-masks', 'safety', 'protection', 'public-health', 'prevention'],\n",
    "    3: ['social-media', 'resilience', 'community-support', 'online-events'],\n",
    "    4: ['global', 'cases', 'deaths', 'statistics'],\n",
    "    5: ['politics', 'government-response', 'conspiracy', 'human-rights'],\n",
    "    6: ['health', 'information','vacccine', 'public-awareness'],\n",
    "    7: ['layoffs', 'misinformation', 'mental-health', 'lockdown', 'access', 'financial-impact', 'political-response', 'education']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a466e279",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Add the labels as ground truth\n",
    "df['cluster'] = kmeans.predict(df)\n",
    "\n",
    "# Add a column caled labels which is a list of strings\n",
    "df['labels'] = df['cluster'].apply(lambda x: labels[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8f52ab97",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['labels'] = df['labels'].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8acc7f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(kmeans, open('../trained_models/kmeans.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4377763",
   "metadata": {},
   "source": [
    "### Sample 5k Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d60e68d2",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Sample Rows\n",
    "df_sampled = df.sample(n = 10_000, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0fb1f92e",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Split the data 70/30\n",
    "train, test = train_test_split(df_sampled, test_size = 0.3)\n",
    "\n",
    "# Remove non-training cols and split to X,y\n",
    "drop_cols = ['cluster', 'labels']\n",
    "X_train, X_test = train.drop(drop_cols, axis = 1), test.drop(drop_cols, axis = 1)\n",
    "\n",
    "# One-hot encode the list of labels to multioutputs\n",
    "y_train = train['labels'].str.get_dummies(sep =' ')\n",
    "y_test = test['labels'].str.get_dummies(sep =' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2efca7",
   "metadata": {},
   "source": [
    "### Compile Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cb68824e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultiOutputClassifier(estimator=LogisticRegression())</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultiOutputClassifier</label><div class=\"sk-toggleable__content\"><pre>MultiOutputClassifier(estimator=LogisticRegression())</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultiOutputClassifier(estimator=LogisticRegression())"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = MultiOutputClassifier(LogisticRegression())\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bf9856b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(lr, open('../trained_models/lr_v2.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7645283c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = lr.predict(X_train)\n",
    "test_pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "aeb855c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.03093040293040293, 0.03643589743589744)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hamming_loss(train_pred, y_train), hamming_loss(test_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48d8afab",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def compute_performance(model: Callable, \n",
    "                        X_train: pd.DataFrame, \n",
    "                        y_train: Union[pd.DataFrame, pd.Series], \n",
    "                        X_test: pd.DataFrame,\n",
    "                        y_test: Union[pd.DataFrame, pd.Series]): \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Computes the performance of a given model for training and testing\n",
    "    Params: \n",
    "        - model (Callable): any sklearn callable \n",
    "        - rest are obvious\n",
    "    Returns: \n",
    "        - results (dict): dict of form {metric1: value1, metric2: value2 ....} of the performance metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Make predictions \n",
    "    train_pred = model.predict(X_train)\n",
    "    test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Hamming Score \n",
    "    train_hamming = hamming_loss(train_pred, y_train)\n",
    "    test_hamming = hamming_loss(test_pred, y_test)\n",
    "    \n",
    "    # Accuracy \n",
    "    train_acc = accuracy_score(train_pred, y_train)\n",
    "    test_acc = accuracy_score(test_pred, y_test)\n",
    "    \n",
    "    \n",
    "    # Compile results \n",
    "    metric_names = ['hamming_loss', 'accuracy']\n",
    "    train_metrics = [train_hamming, train_acc]\n",
    "    test_metrics = [test_hamming, test_acc]\n",
    "    \n",
    "    results = {}\n",
    "    for metric, train_value, test_value in zip(metric_names, train_metrics, test_metrics): \n",
    "        \n",
    "        results.update({\n",
    "            f'train_{metric}': round(train_value, 4),\n",
    "            f'test_{metric}': round(test_value, 4)\n",
    "        })\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c6e7c31",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../trained_models/mlp.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m results \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_name \u001b[38;5;129;01min\u001b[39;00m model_names: \n\u001b[1;32m      5\u001b[0m     \n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Load model\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     model \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../trained_models/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# Compute performance \u001b[39;00m\n\u001b[1;32m     11\u001b[0m     performance \u001b[38;5;241m=\u001b[39m compute_performance(model, X_train, y_train, X_test, y_test)\n",
      "File \u001b[0;32m~/miniconda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    277\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m     )\n\u001b[0;32m--> 282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../trained_models/mlp.pkl'"
     ]
    }
   ],
   "source": [
    "# Compute model performance \n",
    "model_names = ['LR', 'LDA', 'MLP', 'RF', 'GBC']\n",
    "results = {}\n",
    "for model_name in model_names: \n",
    "    \n",
    "    # Load model\n",
    "    model = pickle.load(open(f'../trained_models/{model_name.lower()}.pkl', 'rb'))\n",
    "\n",
    "    \n",
    "    # Compute performance \n",
    "    performance = compute_performance(model, X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    # Update results object\n",
    "    results.update({model_name: performance})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d6f7bc48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_hamming_loss</th>\n",
       "      <th>test_hamming_loss</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LR</th>\n",
       "      <td>0.0426</td>\n",
       "      <td>0.0412</td>\n",
       "      <td>0.6829</td>\n",
       "      <td>0.6913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDA</th>\n",
       "      <td>0.0285</td>\n",
       "      <td>0.0294</td>\n",
       "      <td>0.8071</td>\n",
       "      <td>0.8033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP</th>\n",
       "      <td>0.2582</td>\n",
       "      <td>0.2625</td>\n",
       "      <td>0.0069</td>\n",
       "      <td>0.0053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF</th>\n",
       "      <td>0.1949</td>\n",
       "      <td>0.1966</td>\n",
       "      <td>0.0046</td>\n",
       "      <td>0.0060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GBC</th>\n",
       "      <td>0.1064</td>\n",
       "      <td>0.1046</td>\n",
       "      <td>0.2143</td>\n",
       "      <td>0.2247</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     train_hamming_loss  test_hamming_loss  train_accuracy  test_accuracy\n",
       "LR               0.0426             0.0412          0.6829         0.6913\n",
       "LDA              0.0285             0.0294          0.8071         0.8033\n",
       "MLP              0.2582             0.2625          0.0069         0.0053\n",
       "RF               0.1949             0.1966          0.0046         0.0060\n",
       "GBC              0.1064             0.1046          0.2143         0.2247"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to D \n",
    "pd.DataFrame(results).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d344104",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
