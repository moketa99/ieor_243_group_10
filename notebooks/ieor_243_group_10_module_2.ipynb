{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c9c883b",
   "metadata": {},
   "source": [
    "# IEOR 243 Group 10 Module 2\n",
    "\n",
    "\n",
    "We want to address the following questions through code and our report:\n",
    "\n",
    "#### What is the goal of our models and what are you trying to achieve?\n",
    "\n",
    "Our general goal is to understand how people felt about COVID in 2020 and compare it to how people felt about COVID in 2023. We also want to be more specific than a simple sentiment analysis in which we score sentiment on a -10 to 10 scale (or something comparable). We want to be able to capture the full range of emotions that people convey about COVID. For instance, people may be sad about losing a loved one, scared about getting sick or losing money, angry (or happy) about policy created by legislators, etc. These emotions should not be abstracted into a single number necessarily. \n",
    "\n",
    "#### What models and methodologies have you used?\n",
    "\n",
    "We are in an interesting scenario because our data is *not labeled* meaning we cannot train any supervised learning models to predict sentiment, emotions, etc. We have over 100k rows so manually labeling the data is infeasible. Instead, we introduce three modeling approaches to analyze our problem.\n",
    "\n",
    "1) **K-Means Clustering**: We begin by creating a K-Means clustering algorithm on the `text` column in our data to understand the general trends of communication. \n",
    "\n",
    "    * **K-Means Clusters as Labels**: We then sample tweets from each cluster, cobmine them together, and run inference on a pre-trained summarization model to arrive as cluster descriptions to use with our Zero-Shot models later.\n",
    "\n",
    "\n",
    "2) **Zero-Shot Classification with Hugging Face SOTA**: Hugging Face hosts many open-source, pre-trained models for sentiment analysis and text-classification. We'll apply a few of these models for zero-shot classification on our data to see how it performs. This is difficult to compare to an unsupservised model, but gives us a more useful model. \n",
    "\n",
    "\n",
    "3) **Unsupervised Text Classification through Lbl2Vec**: Lbl2Vec is a rather new algorithm published in 2021 (paper: https://www.scitepress.org/Link.aspx?doi=10.5220/0010710300003058) which automatically generates jointly embedded label, document and word vectors and returns documents of categories modeled by manually predefined keywords. The key idea of the algorithm is that many semantically similar keywords can represent a category. We'll compare this to the zero-shot learning from Hugging Face, along with our K-Means clustering to see how it performsn.\n",
    "\n",
    "#### How have you selected any tuning parameters for your models (a.k.a hyperparameters)?\n",
    "\n",
    "While most supervised learning algorithms benefit greatly from using a standard K-Fold cross-validation implementation using Scikit-Learn's `GridSearchCV` object, for K-Means clustering, it's quite common to use the \"elbow method.\" Instead of directly minimizing some evaluation metric like SSE or Inertia, we instead will search over a space of reasonable values of `n_clusters` and plot the corresponding silhouette scores. We will select a quasi-optimal value of `n_clusters` by selecting the one on the \"elbow\" of the plot.\n",
    "\n",
    "#### How well does your model perform and how have you measured that?\n",
    "\n",
    "\n",
    "#### Are there any significant limitations to your modeling methods (i.e. Where, when, how, and why can your model fail? )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e25b0fe4",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# General Imports \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcd68faf",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/bennettcohen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/bennettcohen/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLTK Imports \n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "from nltk import SnowballStemmer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a5429a2",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Modeling Imports\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744faf40",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "First, we load in the uncleaned data and apply the data cleaning steps from Module 1. Please refer to the `ieor_243_group_10_module_1.ipynb` notebook for an in-depth explanation of this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "562879f1",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Load data \n",
    "parent_path = \"../data\"\n",
    "df20 = pd.read_csv(f\"{parent_path}/covid2020.csv\")\n",
    "df23 = pd.read_csv(f\"{parent_path}/covid2023.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85ceaeeb",
   "metadata": {
    "code_folding": [
     1,
     53,
     76,
     79,
     95,
     110
    ]
   },
   "outputs": [],
   "source": [
    "# Preprocessing Helper Functions\n",
    "def preprocess_text(text: str) -> str:\n",
    "    \n",
    "    \"\"\"\n",
    "    Processes a tweet string by removing any weird string characters/formattings\n",
    "    Args: \n",
    "        - text (str): the text to clean\n",
    "    Returns: \n",
    "        - clean_text (str): the cleaned text string\n",
    "    \"\"\"\n",
    "    # convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # remove URLs\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    \n",
    "    # remove numbers\n",
    "    text = re.sub(r\"\\d\", \"\", text)\n",
    "    \n",
    "    # remove punctuation\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    \n",
    "    # remove stopwords\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Removing extra spaces\n",
    "    text = \" \".join(tokens)\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    \n",
    "    # Removing Emojis\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "    \n",
    "    # Removing emoticons\n",
    "    text = re.sub(r':\\w+:', '', text)\n",
    "    \n",
    "    # Removing Contractions\n",
    "    text = re.sub(r\"can\\'t\", \"can not\", text)\n",
    "    text = re.sub(r\"won\\'t\", \"will not\", text)\n",
    "    text = re.sub(r\"n\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'s\", \" is\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'m\", \" am\", text)\n",
    "    \n",
    "    clean_text = text\n",
    "    \n",
    "    return clean_text\n",
    "\n",
    "def preprocess_nulls(df: pd.DataFrame) -> pd.DataFrame: \n",
    "    \n",
    "    \"\"\"\n",
    "    Removes nulls and 0 counts from a dataframe\n",
    "    Args: \n",
    "        - df (pd.DataFrame): the dataframe to remove nulls from\n",
    "    Returns: \n",
    "        - clean_df (str): the cleaned df\n",
    "    \"\"\"\n",
    "    \n",
    "    # Drop duplicate rows \n",
    "    df = df.drop_duplicates(subset = \"text\")\n",
    "    \n",
    "    # Drop rows with no followers \n",
    "    df = df[df['user_followers'] > 0]\n",
    "    \n",
    "    # Drop nulls and reset index \n",
    "    df = df.dropna().reset_index(drop = True)\n",
    "    \n",
    "    clean_df = df\n",
    "    \n",
    "    return clean_df\n",
    "\n",
    "def remove_emojis(column):\n",
    "    return re.sub(r'[^\\x00-\\x7F]+', '', column)\n",
    "\n",
    "def stem_text(stemmer: SnowballStemmer, text: str) -> str:\n",
    "    \n",
    "    \"\"\"\n",
    "    Applies the NLTK snowball stemmer to a text string\n",
    "    Args: \n",
    "        - stemmer (SnowballStemmer): the stemmer object from NLTK to ouse\n",
    "        - text (str): the string of text to stem\n",
    "    Returns: \n",
    "        - stemmed_text (str): the text with all stemmed words\"\"\"\n",
    "    \n",
    "    words = text.split()\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    stemmed_text =  \" \".join(stemmed_words)\n",
    "\n",
    "    return stemmed_text\n",
    "\n",
    "def get_short_long_words(list_of_tokens: List[str]):\n",
    "    \n",
    "    \"\"\"\n",
    "    Extracts words from list of tokens that are <2 letters and > 21 letters\n",
    "    Args: \n",
    "        - list_of_tokens (List[str]): a list of tokens to clean\n",
    "    Returns: \n",
    "        - two_letter_words (List[str]): a list of tokens to remove\"\"\"\n",
    "    \n",
    "    two_letter_words = []\n",
    "    for token in list_of_tokens:\n",
    "        if len(token) <= 2 or len(token) >= 21:\n",
    "            two_letter_words.append(token)\n",
    "    return two_letter_words\n",
    "\n",
    "def preprocess_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \n",
    "    \"\"\"\n",
    "    Main processing function on the dataframe\n",
    "    Args: \n",
    "        - df (pd.DataFrame): df of tweets to process\n",
    "    Returns: \n",
    "        - preprocessed_df (pd.DataFrame): the processed df\n",
    "    \"\"\"\n",
    "    \n",
    "    # Preprocess null and missing values \n",
    "    df = preprocess_nulls(df)\n",
    "    \n",
    "    # Preprocess text \n",
    "    df['processed_text'] = df['text'].apply(preprocess_text)\n",
    "    \n",
    "    # Remove emojis from certain columns\n",
    "    emoji_cols = ['user_name', 'user_location', 'user_description']\n",
    "    for col in emoji_cols: \n",
    "        df[col] = df[col].apply(remove_emojis)\n",
    "            \n",
    "    # Apply word stemming\n",
    "#     stemmer = SnowballStemmer(language = \"english\")\n",
    "#     df['text'] = df['text'].apply(lambda x: stem_text(stemmer = stemmer, text = x))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5cc75c0",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Process the data and extract text columns\n",
    "df20 = preprocess_df(df20)\n",
    "df23 = preprocess_df(df23)\n",
    "\n",
    "X20 = df20['processed_text']\n",
    "X23 = df23['processed_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acb783b",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69c5bf89",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Apply a TFIDF Vectorizer (count vec with normalization) \n",
    "vectorizer = TfidfVectorizer(max_df = 0.5, min_df = 5, stop_words = 'english')\n",
    "X20_vec = vectorizer.fit_transform(X20)\n",
    "X23_vec = vectorizer.fit_transform(X23)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64562a9c",
   "metadata": {},
   "source": [
    "## Method #1: K-Means Clustering\n",
    "\n",
    "We search over a space `n_cluster` values from 2 to 9 and use the elbow method to select the value with the highest `silhouette_score` as our final model. We then plot the results below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58cd642c",
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_kmeans(X, n_cluster_values: List[int], verbose: bool = True) -> dict: \n",
    "    \n",
    "    \"\"\"\n",
    "    Function to train KMeans and run a parameter sweep over n_cluster_values \n",
    "    Params: \n",
    "        - X: data to train on\n",
    "        - n_cluster_values: list of integers of the number of clusters to try\n",
    "        - random_seed: random seed to pass to KMeans estimator \n",
    "        v- erbose: bool\n",
    "    Returns: \n",
    "        - scores: dictionary of silhouette scores for each n_clusters value\n",
    "    \"\"\"\n",
    "    \n",
    "    # Search over a search space of n_cluster values \n",
    "    scores = {}\n",
    "    tic = time.time()\n",
    "    for n_clusters in n_cluster_values: \n",
    "\n",
    "        # Fit a model \n",
    "        kmeans = KMeans(n_clusters = n_clusters, random_state = 1)\n",
    "        kmeans.fit(X)\n",
    "\n",
    "\n",
    "        # Compute the silhouette score\n",
    "        score = silhouette_score(X = X, \n",
    "                                 labels = kmeans.labels_, \n",
    "                                 metric = 'euclidean'\n",
    "                                )\n",
    "        scores.update({n_clusters: score})\n",
    "\n",
    "        # Logging \n",
    "        if verbose: \n",
    "            print(f\"n_clusters: {n_clusters} -- {time.time()-tic:2f} seconds\")\n",
    "            \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37345727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_clusters: 2 -- 261.497198 seconds\n",
      "n_clusters: 3 -- 493.354532 seconds\n",
      "n_clusters: 4 -- 655.234034 seconds\n",
      "n_clusters: 5 -- 998.278643 seconds\n",
      "n_clusters: 6 -- 1190.112508 seconds\n"
     ]
    }
   ],
   "source": [
    "# Train the KMeans Estimator \n",
    "scores = train_kmeans(X = X20_vec, \n",
    "                      n_cluster_values = list(range(2, 8)), \n",
    "                      verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ab49e8",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Plot the Silhouette score results\n",
    "fig, ax = plt.subplots(figsize = (8,5))\n",
    "\n",
    "n_cluster_vals = list(scores.keys())\n",
    "n_cluster_scores = list(scores.values())\n",
    "\n",
    "plt.plot(n_cluster_vals, n_cluster_scores)\n",
    "\n",
    "plt.title(\"Silhouette Score vs Number of Clusters\", pad = 15, fontweight = 'semibold', fontsize = 14)\n",
    "plt.xlabel(\"Number of Clusters\", fontsize = 13)\n",
    "plt.ylabel(\"Silhouette Score\", fontsize = 13)\n",
    "\n",
    "plt.xticks(n_cluster_vals, n_cluster_vals)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4acb1a16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KMeans(max_iter=5000, n_clusters=6, random_state=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KMeans</label><div class=\"sk-toggleable__content\"><pre>KMeans(max_iter=5000, n_clusters=6, random_state=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KMeans(max_iter=5000, n_clusters=6, random_state=1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the best model and retrain it\n",
    "# optimal_n_clusters = list(scores.keys())[np.argmax(list(scores.values()))]\n",
    "optimal_n_clusters = 6\n",
    "kmeans = KMeans(n_clusters = optimal_n_clusters, random_state = 1, max_iter = 5_000)\n",
    "kmeans.fit(X20_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d25d21d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    0.809949\n",
       "4    0.075538\n",
       "1    0.064597\n",
       "0    0.032734\n",
       "2    0.013150\n",
       "5    0.004032\n",
       "Name: cluster, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add predictions to df and check the cluster distribution\n",
    "df20['cluster'] = kmeans.predict(X20_vec)\n",
    "df20['cluster'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52090ca7",
   "metadata": {},
   "source": [
    "## Method #1a: Using Pretrained Transformers to Generate Class Summaries\n",
    "\n",
    "Now that we have a well trained KMeans clustering algorithm, we want to understand (or label) these clusters. One common approach is to look manually and determine what each cluster entails, but we are more principled than that. Instead, we'll use a transformer-based summarization tool from Hugging Face in order to summarize what each cluster actually discusses. This will give us a clean 1 sentence summary of each cluster that we can use to contextualize our analysis.\n",
    "\n",
    "**NOTE**: We will be running the summarization on the `text` instead of the `processed_text` column because the Hugging Face models generally have their own tokenization functions. We don't want to double-tokenize."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4d2f8c",
   "metadata": {},
   "source": [
    "## USE CHATGPT API TO GENERATE CLASS LABELS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfebf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the summarizer from Hugging Face\n",
    "summarizer = pipeline(\"summarization\", model = \"facebook/bart-large-cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d29455",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "cluster_summaries = {}\n",
    "for cluster in range(kmeans.n_clusters): \n",
    "    \n",
    "    # Grab the tweets from the cluster \n",
    "    cluster_tweets = df20[df20['cluster'] == cluster]['text'].to_list()\n",
    "    \n",
    "    # Randomly Sample tweets\n",
    "    n_samples = np.minimum(len(cluster_tweets), 50)\n",
    "    sampled_tweets = random.sample(cluster_tweets, k = n_samples)\n",
    "    \n",
    "    # Join the sampled tweets \n",
    "    sampled_tweets_joined = \";\".join(sampled_tweets)\n",
    "    \n",
    "    # Run the summarizer OR JUST USE CHATGPT\n",
    "    res = summarizer(sampled_tweets_joined, max_length = 25, min_length = 25, do_sample = False)\n",
    "    \n",
    "    # Update the dict \n",
    "    cluster_summaries.update({cluster: res[0]['summary_text']})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8f6536",
   "metadata": {},
   "source": [
    "masks, cases/deaths, policy, economy\n",
    "\n",
    "masks: antimask, liberal, shutdown\n",
    "cases/deaths: fake numbers, deaths by region"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc80e04",
   "metadata": {},
   "source": [
    "## Method #2: Zero-Shot Classification with Hugging Face SOTA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a332fd",
   "metadata": {},
   "source": [
    "https://huggingface.co/facebook/bart-large-mnli"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239339a6",
   "metadata": {},
   "source": [
    "## Visualizing the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf2c713",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7437a034",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538068d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
